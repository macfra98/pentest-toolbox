import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import torch.nn as nn
import torch.optim as optim

def convert_hash_to_numbers(hash, input_size):
    # Convert each character in the hash to a number
    numerical_representation = [int(char, 16) if char.isdigit() else ord(char.lower()) - 87 for char in hash]

    # Ensure the numerical representation is of fixed size (input_size)
    fixed_size_representation = numerical_representation[:input_size]  # Truncate if necessary
    if len(fixed_size_representation) < input_size:
        # Pad with zeros if below input_size
        fixed_size_representation += [0] * (input_size - len(fixed_size_representation))
    
    return fixed_size_representation

class HashDataset(Dataset):
    def __init__(self, filepath, input_size):
        self.hashes = []
        self.labels = []
        self.input_size = input_size

        # Read the file and preprocess data
        with open(filepath, 'r') as file:
            for line in file:
                hash, label = line.strip().split(',')  # Adjust split character if necessary
                self.hashes.append(convert_hash_to_numbers(hash, self.input_size))
                self.labels.append(self.encode_label(label))

    def encode_label(self, label):
        # Implement label encoding logic here
        # Example: Integer encoding
        label_to_int = {'MD5': 0, 'SHA-1': 1, 'SHA-256': 2, 'SHA-512': 3,
                        'SHA-224': 4, 'SHA-384': 5, 'BLAKE2': 6,
                        'BCRYPT': 7}  # Extend this as necessary
        return label_to_int[label]

    def __len__(self):
        return len(self.hashes)

    def __getitem__(self, idx):
        # Convert lists to PyTorch tensors
        return torch.tensor(self.hashes[idx], dtype=torch.float), torch.tensor(self.labels[idx], dtype=torch.long)

class HashClassifier(nn.Module):
    def __init__(self, input_size, num_classes):
        super(HashClassifier, self).__init__()
        self.layer1 = nn.Linear(input_size, 128)
        self.layer2 = nn.Linear(128, 256)
        self.layer3 = nn.Linear(256, num_classes)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.layer1(x))
        x = self.relu(self.layer2(x))
        x = self.layer3(x)
        return x

# Hyperparameters
input_size = 64  # Adjust according to your hash length after preprocessing
num_classes = 8  # Adjust according to the number of hash types you have
learning_rate = 0.0001
batch_size = 2
num_epochs = 30

def custom_collate_fn(batch):
    # Separate sequences and labels
    sequences, labels = zip(*batch)

    # Pad sequences
    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)

    # Stack labels
    labels_stacked = torch.stack(labels, dim=0)

    return sequences_padded, labels_stacked

# Load Data
train_dataset = HashDataset('data/hashes.txt', input_size)
test_dataset = HashDataset('data/hashes.txt', input_size)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)

# Initialize network
model = HashClassifier(input_size=input_size, num_classes=num_classes)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Train the model
for epoch in range(num_epochs):
    model.train()
    for i, (hashes, labels) in enumerate(train_loader):
        # Forward pass
        outputs = model(hashes)
        loss = criterion(outputs, labels)

        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (i+1) % 100 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')
    
    # Validation phase
    model.eval()
    total = 0
    correct = 0
    with torch.no_grad():
        for hashes, labels in test_loader:
            outputs = model(hashes)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print(f'Accuracy of the model on the test hashes: {100 * correct / total}%')

torch.save(model.state_dict(), 'models/model.pth')
print('Model saved to models/model.pth')

##################################################
############ TEST ################################
##################################################
def test_single_hash(input_hash, input_size):
    numerical_hash = convert_hash_to_numbers(input_hash, input_size)
    hash_tensor = torch.tensor([numerical_hash], dtype=torch.float)

    model = HashClassifier(input_size, num_classes)  # Initialize the model
    model.load_state_dict(torch.load('models/model.pth'))  # Load the trained model weights
    model.eval()  # Set the model to evaluation mode

    with torch.no_grad():  # Disable gradient computation for inference
        output = model(hash_tensor)
        probabilities = torch.softmax(output, dim=1)
        predicted_class = torch.argmax(probabilities, dim=1)  # Get the class with the highest probability

    idx_to_label = {0: 'MD5', 1: 'SHA-1', 2: 'SHA-256', 3: 'SHA-512',
                    4: 'SHA-224', 5: 'SHA-384', 6: 'BLAKE2',
                    7: 'BCRYPT'}
    hash_type = idx_to_label[predicted_class.item()]
    print(f'The hash is predicted to be of type: {hash_type}')

# Test the model with a single hash
input_hash = "7fbc1449e1b4908d3c6373f2ad81d37e"  # Example SHA-256 hash
test_single_hash(input_hash, input_size)

